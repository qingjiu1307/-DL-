{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据集\n",
    "使用相同数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "\n",
    "x = nd.random_normal(shape = (num_examples, num_inputs))\n",
    "y = true_w[0] * x[:, 0] + true_w[1] * x[:, 1] + true_b\n",
    "y += 0.01 * nd.random_normal(shape = y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "dataset = gluon.data.ArrayDataset(x,y)\n",
    "data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型\n",
    "用Sequential把所有的层串接起来,先定义的是一个空的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "而后加入一个Dense全连接层，唯一需要定义的参数是输出节点个数，因为输入节点可根据输入计算得到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.add(gluon.nn.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dense(None -> 1, linear)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数\n",
    "使用Sequential所带有的方法进行初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数\n",
    "gluon提供了平方损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "square_loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化\n",
    "进行随机梯度下降，创建一个Trainer实例， 并且将模型参数传过去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate' : 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n",
    "这里不再调用SGC, 而是使用trainer.step来更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 0, average loss: 0.671321\n",
      "Epochs: 1, average loss: 0.000049\n",
      "Epochs: 2, average loss: 0.000049\n",
      "Epochs: 3, average loss: 0.000049\n",
      "Epochs: 4, average loss: 0.000049\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 10\n",
    "for e in range(epochs):\n",
    "    total_loss = 0\n",
    "    for data,label in data_iter:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = square_loss(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        total_loss += nd.sum(loss).asscalar()\n",
    "    print(\"Epochs: %d, average loss: %f\" % (e, total_loss/num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从net中拿到需要的层，访问权重和位移 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.999774  -3.4003794]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = net[0]\n",
    "dense.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[4.2001123]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.bias.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Parameter in module mxnet.gluon.parameter object:\n",
      "\n",
      "class Parameter(builtins.object)\n",
      " |  A Container holding parameters (weights) of Blocks.\n",
      " |  \n",
      " |  :py:class:`Parameter` holds a copy of the parameter on each :py:class:`Context` after\n",
      " |  it is initialized with ``Parameter.initialize(...)``. If :py:attr:`grad_req` is\n",
      " |  not ``'null'``, it will also hold a gradient array on each :py:class:`Context`::\n",
      " |  \n",
      " |      ctx = mx.gpu(0)\n",
      " |      x = mx.nd.zeros((16, 100), ctx=ctx)\n",
      " |      w = mx.gluon.Parameter('fc_weight', shape=(64, 100), init=mx.init.Xavier())\n",
      " |      b = mx.gluon.Parameter('fc_bias', shape=(64,), init=mx.init.Zero())\n",
      " |      w.initialize(ctx=ctx)\n",
      " |      b.initialize(ctx=ctx)\n",
      " |      out = mx.nd.FullyConnected(x, w.data(ctx), b.data(ctx), num_hidden=64)\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  name : str\n",
      " |      Name of this parameter.\n",
      " |  grad_req : {'write', 'add', 'null'}, default 'write'\n",
      " |      Specifies how to update gradient to grad arrays.\n",
      " |  \n",
      " |      - ``'write'`` means everytime gradient is written to grad :py:class:`NDArray`.\n",
      " |      - ``'add'`` means everytime gradient is added to the grad :py:class:`NDArray`. You need\n",
      " |        to manually call ``zero_grad()`` to clear the gradient buffer before each\n",
      " |        iteration when using this option.\n",
      " |      - 'null' means gradient is not requested for this parameter. gradient arrays\n",
      " |        will not be allocated.\n",
      " |  shape : tuple of int, default None\n",
      " |      Shape of this parameter. By default shape is not specified. Parameter with\n",
      " |      unknown shape can be used for :py:class:`Symbol` API, but ``init`` will throw an error\n",
      " |      when using :py:class:`NDArray` API.\n",
      " |  dtype : numpy.dtype or str, default 'float32'\n",
      " |      Data type of this parameter. For example, ``numpy.float32`` or ``'float32'``.\n",
      " |  lr_mult : float, default 1.0\n",
      " |      Learning rate multiplier. Learning rate will be multiplied by lr_mult\n",
      " |      when updating this parameter with optimizer.\n",
      " |  wd_mult : float, default 1.0\n",
      " |      Weight decay multiplier (L2 regularizer coefficient). Works similar to lr_mult.\n",
      " |  init : Initializer, default None\n",
      " |      Initializer of this parameter. Will use the global initializer by default.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  grad_req : {'write', 'add', 'null'}\n",
      " |      This can be set before or after initialization. Setting ``grad_req`` to ``'null'``\n",
      " |      with ``x.grad_req = 'null'`` saves memory and computation when you don't\n",
      " |      need gradient w.r.t x.\n",
      " |  lr_mult : float\n",
      " |      Local learning rate multiplier for this Parameter. The actual learning rate\n",
      " |      is calculated with ``learning_rate * lr_mult``. You can set it with\n",
      " |      ``param.lr_mult = 2.0``\n",
      " |  wd_mult : float\n",
      " |      Local weight decay multiplier for this Parameter.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, name, grad_req='write', shape=None, dtype=<class 'numpy.float32'>, lr_mult=1.0, wd_mult=1.0, init=None, allow_deferred_init=False, differentiable=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  cast(self, dtype)\n",
      " |      Cast data and gradient of this Parameter to a new data type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or numpy.dtype\n",
      " |          The new data type.\n",
      " |  \n",
      " |  data(self, ctx=None)\n",
      " |      Returns a copy of this parameter on one context. Must have been\n",
      " |      initialized on this context before.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ctx : Context\n",
      " |          Desired context.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      NDArray on ctx\n",
      " |  \n",
      " |  grad(self, ctx=None)\n",
      " |      Returns a gradient buffer for this parameter on one context.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ctx : Context\n",
      " |          Desired context.\n",
      " |  \n",
      " |  initialize(self, init=None, ctx=None, default_init=<mxnet.initializer.Uniform object at 0x0000025DEAEE2160>, force_reinit=False)\n",
      " |      Initializes parameter and gradient arrays. Only used for :py:class:`NDArray` API.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      init : Initializer\n",
      " |          The initializer to use. Overrides :py:meth:`Parameter.init` and default_init.\n",
      " |      ctx : Context or list of Context, defaults to :py:meth:`context.current_context()`.\n",
      " |          Initialize Parameter on given context. If ctx is a list of Context, a\n",
      " |          copy will be made for each context.\n",
      " |      \n",
      " |          .. note::\n",
      " |              Copies are independent arrays. User is responsible for keeping\n",
      " |              their values consistent when updating.\n",
      " |              Normally :py:class:`gluon.Trainer` does this for you.\n",
      " |      \n",
      " |      default_init : Initializer\n",
      " |          Default initializer is used when both :py:func:`init`\n",
      " |          and :py:meth:`Parameter.init` are ``None``.\n",
      " |      force_reinit : bool, default False\n",
      " |          Whether to force re-initialization if parameter is already initialized.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> weight = mx.gluon.Parameter('weight', shape=(2, 2))\n",
      " |      >>> weight.initialize(ctx=mx.cpu(0))\n",
      " |      >>> weight.data()\n",
      " |      [[-0.01068833  0.01729892]\n",
      " |       [ 0.02042518 -0.01618656]]\n",
      " |      <NDArray 2x2 @cpu(0)>\n",
      " |      >>> weight.grad()\n",
      " |      [[ 0.  0.]\n",
      " |       [ 0.  0.]]\n",
      " |      <NDArray 2x2 @cpu(0)>\n",
      " |      >>> weight.initialize(ctx=[mx.gpu(0), mx.gpu(1)])\n",
      " |      >>> weight.data(mx.gpu(0))\n",
      " |      [[-0.00873779 -0.02834515]\n",
      " |       [ 0.05484822 -0.06206018]]\n",
      " |      <NDArray 2x2 @gpu(0)>\n",
      " |      >>> weight.data(mx.gpu(1))\n",
      " |      [[-0.00873779 -0.02834515]\n",
      " |       [ 0.05484822 -0.06206018]]\n",
      " |      <NDArray 2x2 @gpu(1)>\n",
      " |  \n",
      " |  list_ctx(self)\n",
      " |      Returns a list of contexts this parameter is initialized on.\n",
      " |  \n",
      " |  list_data(self)\n",
      " |      Returns copies of this parameter on all contexts, in the same order\n",
      " |      as creation.\n",
      " |  \n",
      " |  list_grad(self)\n",
      " |      Returns gradient buffers on all contexts, in the same order\n",
      " |      as :py:meth:`values`.\n",
      " |  \n",
      " |  reset_ctx(self, ctx)\n",
      " |      Re-assign Parameter to other contexts.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ctx : Context or list of Context, default ``context.current_context()``.\n",
      " |          Assign Parameter to given context. If ctx is a list of Context, a\n",
      " |          copy will be made for each context.\n",
      " |  \n",
      " |  set_data(self, data)\n",
      " |      Sets this parameter's value on all contexts.\n",
      " |  \n",
      " |  var(self)\n",
      " |      Returns a symbol representing this parameter.\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradient buffer on all contexts to 0. No action is taken if\n",
      " |      parameter is uninitialized or doesn't require gradient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  grad_req\n",
      " |  \n",
      " |  shape\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dense.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.01373221  0.0192259 ]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.weight.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
